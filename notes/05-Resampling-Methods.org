* 5 Resampling Methods

- Resampling methods :: repeatedly drawing samples from a training set
     and refiting a model of interest on each sample, to learn more
     about the fitted model.

- Computationally expensive

- Examples:
  - Cross validation, used for /model assessment/ or /model selection/.
  - Bootstrap, used for a measure of accuracy of a parameter estimate.

* 5.1 Cross-Validation

- recall: test error rate vs training error rate.

** 5.1.1 The Validation Set Approach

Separate the data into two sets, the /training set/ and the
/validation set/.

- validation set :: A portion of the training data reserved (not used
                    in training the model) to be used only for
                    estimating the test error.

- Validation sets can be used to do model selection.

- Cons:
  - Estimated test error rate can be highly variable
  - Only a subset of the observations are used to fit the model.

** 5.1.2 Leave-One-Out Cross-Validation

Closely related to the validation set approach, but attempts to
address the method's drawbacks.

- Leave one out :: Separate the training data into two sets, but only
                   include one observation in the validation set.

We can calculate an estimate for the test MSE, $CV_{(n)}$ with the
follow procedure:

1. Use the leave-one-out approach for every element $i$
2. estimate $MSE_i = (y_i - \hat{y}_i)^2$
3. Take the average MSE:

\begin{equation}
  CV_{(n)} = \frac{1}{n} \sum_{i=1}^n MSE_i
\end{equation}

Benefits over the validation set approach:

- Less bias (will not overestimate the test error)
- Repeatable

** 5.1.3 k-Fold Cross-Validation

- k-fold CV :: randomly devide the set of observations into k groups
               (folds) of approximately equal size, and treat each fold
               as a test set in one iteration of training/testing.

To estimate MSE, use the same procedure as in leave-one-out, but with
each fold.

- LOOCV is a special case of k-fold CV, in which $k=n$.
- Fitting fewer times can save a lot of computation.
- Still much less variance than the test-set approach

*Important*: Even when our test MSE estimate is not close in absolute
value to the true test MSE, it still often correctly identifies the
"correct" level of flexibility for our model.

- not great when we want an estimate of prediction error
- great when we're doing model selection

** 5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation

There is a bias variance trade off with the number of folds in your
validation.

#+BEGIN_SRC
test set cv      k-fold CV           LOOCV
   k=1 -----------> k=5 ------------> k=n
 high bias                        high variance
#+END_SRC

- We're talking about bias/variance in regards to estimating the test
  MSE

- With LOOCV, we're fitting n models all of which are very similar to
  eachother.

** 5.1.5 Cross-Validation on Classification Problems

When $y$ is qualitative, we can still do CV. Instead of MSE, we use
the number of miss-classified observations:

\begin{equation}
  CV_{(n)} = \frac{1}{n} \sum_{i=1}^n Err_i,
\end{equation}

Where $Err_i = I(y_i \neq \hat{y}_i)$.

* 5.2 The Bootstrap

A statistical tool used to quantify the uncertainty associated with a
given estimator.

To find the variability of an estimator, we could sample the
population repeatedly to find training data, and observe how the
estimator changes. This often isn't feasible. Instead, we can
bootstrap:

- bootstrap :: draw samples from the training data to show the
               variation in our estimator.

Bootstrap procedure

1. Sample from the training data, *with replacement*, call this $Z^{*1}$
2. Create an estimate for our estimator $\hat{\alpha}$ using this bootstrap,
   call it $\hat{\alpha}^{*1}$
3. Repeat steps (1) & (2) $B$ times, for some large value of B.
4. Compute the standard error of these estimates:

\begin{equation}
  SE_b(\hat{\alpha})
  = \sqrt{
      \frac{1}{B - 1} \sum_{r=1}^B
        \left(
          \hat{\alpha}^{*r} - \frac{1}{B} \sum_{r'=1}^B \hat{\alpha}^{*r'}
        \right)^2
  }
\end{equation}

* 5.3 Lab: Cross-Validation and the Bootstrap
** 5.3.1 The Validation Set Approach
** 5.3.2 Leave-One-Out Cross-Validation
** 5.3.3 k-Fold Cross-Validation
** 5.3.4 The Bootstrap
