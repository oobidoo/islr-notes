* 02 Statistical Learning

- pg 34. In talking about a test set, they talk about y_0 and x_0, I
  don't understand what they're getting at.

- pg 36, figure 2.12, really understand this.

- pg 37, are we supposed to understand how bayes estimator is implemented?
  - Do I need to understand the bayes error rate?

- pg 50, that pairs equation, how does it work?
  - ALSO SO FUCKING COOL, holy shit
* 03 Linear Regression
- pg 66 - go back and understand these equations.
- pg 67 - What is the relationship between t-value p-value and
  confidence intervals?
- pg 74 - What, mathematicall, does it mean to "hold TV and radio
  fixed"?
- pg 82 - Can we talk about confidence interval vs prediction
  interval?

** Overall questions

- Can we talk about the relationships between RSS, SE, TSS, and the rest
  of those?
  - Write them down
  - Talk about which are independently useful
  - Talk about what they mean in words

- Can we talk about the potential problems?

- Leverage, maybe I should understand this?

- I still want to better understand how collinearity affects models.
  - Does the wildly differen parameter estimate fit with the way that
    a given variable affects another?

** Exercise Questions

- Why the focus on regression without an intercept. It seems, silly?
  - Is it because you can always normalize your response to have mean zero?
  - Still, these questions seem unmotivated

- 14.f -- I don't understand this
* 04 Classification
- pg 131 predict default yes for any individual for whom p(balance) >
  0.5. Do they mean for whom p(default) > 0.5?
  - Answer: The notation $p(balance)$ is shorthand for
    p(default=Yes|balance)

- pg 133 Why do we want to maximize /this/ likelihood function?

- pg 137/138 I feel like I've seen multi-class logistic regression
  very often, am I mistaken? Is it bad practice? Or is it something
  else disguised as logistic regression.

- pg 138 can we write out these probablities and "flipping around"?
  And Bayes Theorem?

- pg 138 Can we talk about this "density function".

- pg 140 Where did $\delta_k(x)$ come from?

- pg 145 can we talk about the confusion matrix, sensitivity and specificity?

- pg 147 This graph seems cool to talk about. Let's talk about the
  impact of adjusting this threshold

- pg 148 Can we talk about the ROC curve too?

- pg 158 How does the `contrasts` function work?

** Sensitivity, Specificity, Precision, Recall, Power, Type I error, Type II error

Can we talk through all these definitions in the following formats?
Come up with mneumonics for them? And why they're useful?


|           |       | True |      |       |
|-----------+-------+------+------+-------|
|           |       | Yes  | No   | Total |
|-----------+-------+------+------+-------|
| Predicted | Yes   | (TP) | (FP) | P^    |
|           | No    | (FN) | (TN) | N^    |
|-----------+-------+------+------+-------|
|           | Total | P    | N    |       |


TP/P :: sensitivity
TN/N :: specificity

TP/P^ :: precision
TP/P :: recall

FP/N :: Type I Error
FN/P :: Type II Error

TP/P :: power

#+BEGIN_SRC text
     Yes             No
+-------------+-------------+
|          No (pred)        |
|         ____|____         |
|        /    |    \        |
|       /     |     \       |
|      |      |      |      |
|      |  Yes (pred) |      |
|      |      |      |      |
|       \     |     /       |
|        \____|____/        |
|             |             |
|             |             |
+-------------+-------------+
#+END_SRC
* 05 Resampling Methods
- pg 184 -- talk about bias variance trade off
  - I need a better demonstration of how this works:

    #+BEGIN_QUOTE
    Since the mean of many highly correlated quantities has higher
    variance than does the mean of many quantities that are not as
    highly correlated.
    #+END_QUOTE

  - What does that quote mean??

- pg 190 -- Bootstrap questions
  - why do we sample with replacement?
  - How big does $B$ need to be?
