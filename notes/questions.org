* 02 Statistical Learning

- pg 34. In talking about a test set, they talk about y_0 and x_0, I
  don't understand what they're getting at.

- pg 36, figure 2.12, really understand this.

- pg 37, are we supposed to understand how bayes estimator is implemented?
  - Do I need to understand the bayes error rate?

- pg 50, that pairs equation, how does it work?
  - ALSO SO FUCKING COOL, holy shit
* 03 Linear Regression
- pg 66 - go back and understand these equations.
- pg 67 - What is the relationship between t-value p-value and
  confidence intervals?
- pg 74 - What, mathematicall, does it mean to "hold TV and radio
  fixed"?
- pg 82 - Can we talk about confidence interval vs prediction
  interval?

** Overall questions

- Can we talk about the relationships between RSS, SE, TSS, and the rest
  of those?
  - Write them down
  - Talk about which are independently useful
  - Talk about what they mean in words

- Can we talk about the potential problems?

- Leverage, maybe I should understand this?

- I still want to better understand how collinearity affects models.
  - Does the wildly differen parameter estimate fit with the way that
    a given variable affects another?

** Exercise Questions

- Why the focus on regression without an intercept. It seems, silly?
  - Is it because you can always normalize your response to have mean zero?
  - Still, these questions seem unmotivated

- 14.f -- I don't understand this
* 04 Classification
- pg 131 predict default yes for any individual for whom p(balance) >
  0.5. Do they mean for whom p(default) > 0.5?
  - Answer: The notation $p(balance)$ is shorthand for
    p(default=Yes|balance)

- pg 133 Why do we want to maximize /this/ likelihood function?

- pg 137/138 I feel like I've seen multi-class logistic regression
  very often, am I mistaken? Is it bad practice? Or is it something
  else disguised as logistic regression.

- pg 138 can we write out these probablities and "flipping around"?
  And Bayes Theorem?

- pg 138 Can we talk about this "density function".

- pg 140 Where did $\delta_k(x)$ come from?

- pg 145 can we talk about the confusion matrix, sensitivity and specificity?

- pg 147 This graph seems cool to talk about. Let's talk about the
  impact of adjusting this threshold

- pg 148 Can we talk about the ROC curve too?

- pg 158 How does the `contrasts` function work?

** Sensitivity, Specificity, Precision, Recall, Power, Type I error, Type II error

Can we talk through all these definitions in the following formats?
Come up with mneumonics for them? And why they're useful?


|           |       | True |      |       |
|-----------+-------+------+------+-------|
|           |       | Yes  | No   | Total |
|-----------+-------+------+------+-------|
| Predicted | Yes   | (TP) | (FP) | P^    |
|           | No    | (FN) | (TN) | N^    |
|-----------+-------+------+------+-------|
|           | Total | P    | N    |       |


TP/P :: sensitivity
TN/N :: specificity

TP/P^ :: precision
TP/P :: recall

FP/N :: Type I Error
FN/P :: Type II Error

TP/P :: power

#+BEGIN_SRC text
     Yes             No
+-------------+-------------+
|          No (pred)        |
|         ____|____         |
|        /    |    \        |
|       /     |     \       |
|      |      |      |      |
|      |  Yes (pred) |      |
|      |      |      |      |
|       \     |     /       |
|        \____|____/        |
|             |             |
|             |             |
+-------------+-------------+
#+END_SRC
* 05 Resampling Methods
- pg 184 -- talk about bias variance trade off
  - I need a better demonstration of how this works:

    #+BEGIN_QUOTE
    Since the mean of many highly correlated quantities has higher
    variance than does the mean of many quantities that are not as
    highly correlated.
    #+END_QUOTE

  - What does that quote mean??

- pg 190 -- Bootstrap questions
  - why do we sample with replacement?
  - How big does $B$ need to be?
* 06 Regularization

- pg 211 - what is $d$ in this equation?
  - "Clearly the penalty increases as the number of predictors in the model increaes"
    - Why is this clear?

  - *ANSWER* Ohhh, because $d$ is the number of predictors! Why $d$ though?
- pg 215 - a "tuning" parameter is a meta-parameter, right?
  - Is this going to require another tuning set devision that we'll
    use to select meta-parameters?

- pg 219 - The lasso shrinks parameters to zero. But why?
  - Is it because it doesn't disproportionatly penalize larger
    parameters?

- pg 233 - "We assume that the directions in which X_1,...,X_p show
  the most variation are the directions that are associated with Y"
  - Is this reasonable? When is it not reasonable.

** Lab questions
- Why during CV do we need to do regularization using the training data?
- What the hell is going on with ~model.matrix~?
* 07 Beyond Linearity
- pg 273 How do those /truncated power basis/ functions ensure the
  constraints of smooth derivatives a the knots?
- pg 275 How does a natural spline change the form of the equation
  we're fitting.
- pg 275 How does a spline model affect model interpretability?
- pg 276 figure 7.6, doesn't degrees of freedom = 1 represent a linear
  model? How does this relate to choosing the number of knots? Is
  there any proofe that splines improve the sitation here?
* 08 Tree Based Methods
- pg 309: What is a sub-tree?
  - How do you find a given sub-tree? Like, what nodes do you prune?
    And in what order?

- pg 318 "each bagged tree makes use of around two-thirds of the
  observations"
  - I think I need to review the bootstrap to understand this.
- pg 318 How do OOB (out of bag) predictions work?

- pg 322 "In general statistical learning approaches that /learn slowly/ tend to
  perform well."
  - That's a really interesting assertion. What does it mean??

- pg 323 what does it mean to "add in a shrunken version of the new tree"?
* 10 Unsupervised Learning
- pg 378 - How do I interpret the arrows on this chart?
- pg 379 - "The first principal component loading vector ... is the
  line in p-dimensional space that is closest to the n-observations"
  - The vector from zero?? Or a derived vector?
- pg 395 - Why does single linkage lead to less balanced trees?
- pg 397 - What specifically is the correlation based measure?
